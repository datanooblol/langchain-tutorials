{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3df42f-284b-456e-86db-97d08aaee2c4",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "83bd7b7d-8df3-4840-a5ed-1874eec047c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package import embedding, llm, agent_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b14bd1-a7dd-469d-97c3-577ae5adb930",
   "metadata": {},
   "source": [
    "# Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b7826852-be50-440d-9ae2-9e3bf99a1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b36e93-dbe9-43bf-ae4e-d01542b72b21",
   "metadata": {},
   "source": [
    "# Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b29bde77-3879-4a5f-9e39-cc2063169c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# load and store you secret api key\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf3b8f-86ed-4599-849e-4c9eff328cac",
   "metadata": {},
   "source": [
    "# UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2048403b-97dd-4a8b-b800-1f28d8950fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edfcd6-9e76-4259-b6c4-905adcdf3be7",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab250c7e-bb5c-478b-b78c-68ad81a2c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "chunk_overlap = 0.1\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=int(chunk_size*chunk_overlap)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de204d8-64c7-4968-8299-03ce1d5aaa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"https://arxiv.org/pdf/2404.19553\")\n",
    "docs = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5ade595-77f6-4327-888f-da4565977563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53bfb0-0656-4b55-b535-812ddfb8b2bd",
   "metadata": {},
   "source": [
    "# Create vectorstore obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7221b71-8a57-493b-b722-55b880f21342",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"extending_context_window_llama_3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab043d0-8d64-42ce-9744-39036548fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89951a34-36dd-4bef-9f6a-bcda067d9ed2",
   "metadata": {},
   "source": [
    "# RAG-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd591cf4-3186-447b-b009-90e277777887",
   "metadata": {},
   "source": [
    "## Define Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345f2e26-9242-4bba-87e7-2bd532d1c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful assistant. Use the available context to answer the question.\n",
    "If you can't answer the question, say \"I don't have enough information\" and don't make your own answer.\n",
    "Discard irrelavant information.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b671742-4b00-4441-9d7c-9eb0bb102a0a",
   "metadata": {},
   "source": [
    "## Define Chain with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03cc3af7-a42e-479c-b318-075610c45b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever | format_docs, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94f75e-0b92-453a-882a-abda1c4f5373",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed1008ff-b7cd-44b9-9a20-144fb7ea4db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = llm_chain.invoke({\"question\": \"what does the 'context' in 'long context' refer to?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89ef4f5-9678-475b-aa96-ec9c046bffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can help you with that!\n",
      "\n",
      "The \"context\" in \"long context\" refers to the maximum number of tokens (or words) that a language model can process or understand at once. In other words, it's the length of the input text that the model can handle before making predictions or generating output.\n",
      "\n",
      "In this specific context, there are mentions of extending the context window beyond 2 million tokens ([8]), scaling language models to 128k context ([9]), and evaluating long-context evaluation beyond 100k tokens ([17]). These references suggest that \"context\" is indeed related to the maximum input length that a model can handle.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd8bce9-59a4-4e24-a63c-d8387b937cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = (\n",
    "    {\"context\": RunnablePassthrough() | retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f05b1662-0770-48fa-926b-e5f1fcd728ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = llm_chain.invoke(\"what does the 'context' in 'long context' refer to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ccbc7ab-8c52-4980-b5a7-54dac033df04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can help you with that!\n",
      "\n",
      "The \"context\" in \"long context\" refers to the maximum number of tokens (or words) that a language model can process or understand at once. In other words, it's the length of the input text that the model can handle before making predictions or generating output.\n",
      "\n",
      "In this specific context, there are mentions of extending the context window beyond 2 million tokens ([8]), scaling language models to 128k context ([9]), and evaluating long-context evaluation beyond 100k tokens ([17]). These references suggest that \"context\" is indeed related to the maximum input length that a model can handle.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400f3b9-dbb5-4031-b4ab-8618920ce4b3",
   "metadata": {},
   "source": [
    "# Test idea of LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8446663f-40eb-403e-b59b-07705c5f8d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from package import url, model\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(base_url=url, model=model, format=\"json\", temperature=0)\n",
    "\n",
    "grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = grader_prompt | llm | JsonOutputParser()\n",
    "question = \"What does the 'context' in 'long context' means?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = format_docs(docs)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbba291b-cf2d-436e-aa17-dbd2abbaa045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'no'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What does the fox say?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = format_docs(docs)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d246345-b0a2-4af6-9940-84b11ca02252",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(base_url=url, model=model, temperature=0)\n",
    "\n",
    "rag_chain = rag_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a420f59-cb9e-49f1-a8ea-ba97dc502ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"context\" in \"long context\" refers to the amount of text or information that a language model can process or consider when making predictions or generating text. In this case, it seems to refer to the maximum length of text that a model can handle before its performance degrades.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "question = \"What does the 'context' in 'long context' means?\"\n",
    "docs = retriever.invoke(question)\n",
    "docs = format_docs(docs)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcf6a45d-de77-46ec-b217-866e355187b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(base_url=url, model=model, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8880fe3-a2ee-4fb1-bae0-f159ff9fbd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(base_url=url, model=model, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed012009-4237-4752-808e-ed592d74bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doublebank\\.conda\\envs\\langchain-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(base_url=url, model=model, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "    user question to a vectorstore or web search. Use the vectorstore for questions on Extending Llama-3‚Äôs Context Ten-Fold Overnight. You do not need to be stringent with the keywords \n",
    "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no premable or explanation. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"What does the 'context' in 'long context' means?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = format_docs(docs)\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3214264-29e4-4a84-9bda-26ba1af90171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='[8] Y. Ding, L. L. Zhang, C. Zhang, Y. Xu, N. Shang, J. Xu, F. Yang, and M. Yang. Longrope:\\nExtending llm context window beyond 2 million tokens, 2024.\\n[9] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data engineering for\\nscaling language models to 128k context, 2024.\\n[10] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding, 2021.\\n[11] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\\n[12] D. Li*, R. Shao*, A. Xie, Y. Sheng, L. Zheng, J. E. Gonzalez, I. Stoica, X. Ma, , and H. Zhang.\\nHow long can open-source llms truly promise on context length?, June 2023.\\n[13] OpenAI. Gpt-4 technical report, 2024.', metadata={'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'page': 3, 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240501003931Z', 'modDate': 'D:20240501003931Z', 'trapped': '', '_id': '808a24c58c404cbdabbb4dc97c83dbac', '_collection_name': 'extending_context_window_llama_3'})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c8721a8-da47-4843-8f47-e2161f841af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'web_search'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What does the fox say?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = format_docs(docs)\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c851c65b-a8fc-4ea7-a502-55ce5ae076b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc0eb0e0-e633-4f9a-b462-e99786b0be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "### State\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    print(source)\n",
    "    print(source[\"datasource\"])\n",
    "    if source[\"datasource\"] == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source[\"datasource\"] == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5a6e7d5-4e97-4eff-b30e-0ad798a66d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52516d99-9c1c-4768-b486-54749534e3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "What does the 'context' in 'long context' means?\n",
      "{'datasource': 'vectorstore'}\n",
      "vectorstore\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('The \"context\" in the phrase \"long context\" refers to a sequence of tokens or '\n",
      " 'words that an AI model can process and understand. In this case, it seems to '\n",
      " 'refer to the maximum number of tokens or words that a language model can '\n",
      " 'consider when generating text or making predictions.')\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"What does the 'context' in 'long context' means?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dff50f89-695e-40d3-868e-6861f848043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "What does the fox say?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "'Finished running: websearch:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('According to the context, \"The Fox\" by Ylvis says... nothing! The song is an '\n",
      " 'electronic dance novelty song and viral video that features sounds and '\n",
      " \"noises made by a fox, but it doesn't actually say anything in the classical \"\n",
      " 'sense.')\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"What does the fox say?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9887f26e-12bb-405f-bbfe-1626b4addd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "What is the 5 whys techniques?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "'Finished running: websearch:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('The 5 Whys technique is a problem-solving method that involves asking \"why\" '\n",
      " \"five times to drill down to the root cause of a problem. It's a simple yet \"\n",
      " 'effective way to expose weaknesses in systems or processes and was '\n",
      " 'originally developed by Sakichi Toyota for use within the Toyota Motor '\n",
      " 'Corporation.')\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"What is the 5 whys techniques?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cd7e5e4-a329-4796-9d6b-80f454932c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "What is the 5 whys techniques?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 22.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the 5 whys techniques?',\n",
       " 'generation': 'The 5 Whys technique is an iterative interrogative method used to explore the cause-and-effect relationships underlying a particular problem. It involves asking \"why\" five times to drill down to the root cause of the issue. The technique can be performed using either a fishbone (or Ishikawa) diagram or a tabular format.',\n",
       " 'documents': [Document(page_content=\"Five whys (or 5 whys) is an iterative interrogative technique used to explore the cause-and-effect relationships underlying a particular problem. ... Two primary techniques are used to perform a five whys analysis: the fishbone (or Ishikawa) diagram and a tabular format.\\n5 Whys, or 5Y, is a powerful tool for getting to the root cause of a problem, and an effective way to expose weaknesses in your systems or processes. ... The tool's simplicity gives it great flexibility, too, and 5 Whys combines well with other methods and techniques, such as Root Cause Analysis.\\nTips for Mastering the 5 Whys Technique Encouraging Open Communication. In mastering the 5 Whys Technique as a problem-solving method, creating an environment that fosters open communication is paramount. When team members feel comfortable expressing their perspectives and insights, it leads to a more comprehensive exploration of the underlying ...\\nRoot cause analysis (RCA) is a common process for discovering the origin of a business problem. While there are many RCA problem-solving techniques, one popular and easy technique is the 5 Whys method. Performing a 5 Whys analysis is one of the most efficient ways to both discover the root cause of a problem and ensure that steps are taken to ...\\nStep 2: Select a 5 Whys master for the meeting. The 5 Whys master will lead the discussion, ask the 5 whys, and assign responsibility for the solutions the group comes up with. The rest of those involved will answer those questions and discuss. In our experience, anyone can be a 5 Whys master ‚Äî there are no special qualifications, and it ...\")]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a592ec5a-3ff8-4fdc-9bef-10c04fc0eb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "What is the 5 whys techniques?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bd8b6e6-32d5-4f7a-9965-a1d0cc159aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'generation', 'documents'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b176836-904a-4b91-8541-4b8553ce8278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['page_content', 'metadata', 'type'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['documents'][0].dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6655c8ef-9f83-4b14-a6d9-c28428a5bec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['documents'][0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d88be152-6935-4db5-9021-d4360cf6efd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "What does the 'context' in 'long context' means?\n",
      "{'datasource': 'vectorstore'}\n",
      "vectorstore\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs = {\"question\": \"What does the 'context' in 'long context' means?\"}\n",
    "response = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3746ceec-ae19-4b56-8ebf-9c93e0f51ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://arxiv.org/pdf/2404.19553',\n",
       " 'file_path': 'https://arxiv.org/pdf/2404.19553',\n",
       " 'page': 3,\n",
       " 'total_pages': 5,\n",
       " 'format': 'PDF 1.5',\n",
       " 'title': '',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'producer': 'pdfTeX-1.40.25',\n",
       " 'creationDate': 'D:20240501003931Z',\n",
       " 'modDate': 'D:20240501003931Z',\n",
       " 'trapped': '',\n",
       " '_id': '808a24c58c404cbdabbb4dc97c83dbac',\n",
       " '_collection_name': 'extending_context_window_llama_3'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['documents'][0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "823e83a2-213a-4fde-b011-84ed6a8e58b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "What are the differences between LangChain Agent and LangGraph?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs = {\"question\": \"What are the differences between LangChain Agent and LangGraph?\"}\n",
    "response = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae745c38-bebc-453e-924f-d940b7bfe496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='4. Scalability: Langgraph is designed to handle large-scale language models with billions of parameters, enabling the development of state-of-the-art NLP applications. 5. Open-Source: Langgraph is an open-source project, allowing developers and researchers to collaborate, contribute, and build upon the framework. 6.\\nKey Takeaways. LangGraph is an extension of LangChain, which allows us to build cyclic, stateful, multi-actor agent systems. It implements a graph structure with nodes and edges. The nodes are functions or tools, and the edges are the connections between nodes. Edges are of two types: conditional and normal.\\nü¶úüï∏Ô∏èLangGraph. ‚ö° Building language agents as graphs ‚ö°. Overview . LangGraph is a library for building stateful, multi-actor applications with LLMs. Inspired by Pregel and Apache Beam, LangGraph lets you coordinate and checkpoint multiple chains (or actors) across cyclic computational steps using regular python functions (or JS).The public interface draws inspiration from NetworkX.\\nAnd finally, its persistent, versioned checkpointing system lets you roll back the agent\\'s state, explore other paths, and maintain full control of what is going on. The following sections go into greater detail about how and why all of this works. Core Design¬∂ At its core, LangGraph models agent workflows as state machines.\\nAs a part of the launch, we highlighted two simple runtimes: one that is the equivalent of the AgentExecutor in langchain, and a second that was a version of that aimed at message passing and chat models.\\n It\\'s important to note that these three examples are only a few of the possible examples we could highlight - there are almost assuredly other examples out there and we look forward to seeing what the community comes up with!\\n LangGraph: Multi-Agent Workflows\\nLinks\\nLast week we highlighted LangGraph - a new package (available in both Python and JS) to better enable creation of LLM workflows containing cycles, which are a critical component of most agent runtimes. \"\\nAnother key difference between Autogen and LangGraph is that LangGraph is fully integrated into the LangChain ecosystem, meaning you take fully advantage of all the LangChain integrations and LangSmith observability.\\n As part of this launch, we\\'re also excited to highlight a few applications built on top of LangGraph that utilize the concept of multiple agents.\\n')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b8c05e3-a95a-4d1c-b45d-1cef9506e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "What is the 5 whys techniques?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputs = {\"question\": \"What is the 5 whys techniques?\"}\n",
    "response = app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58bf5718-5a74-4cdc-95fe-ff98e0efcf48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['documents'][0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abfd7c0b-380a-41fb-a9cf-55289647acd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Step 2: Select a 5 Whys master for the meeting. The 5 Whys master will lead the discussion, ask the 5 whys, and assign responsibility for the solutions the group comes up with. The rest of those involved will answer those questions and discuss. In our experience, anyone can be a 5 Whys master ‚Äî there are no special qualifications, and it ...\\nFive whys (or 5 whys) is an iterative interrogative technique used to explore the cause-and-effect relationships underlying a particular problem. ... Two primary techniques are used to perform a five whys analysis: the fishbone (or Ishikawa) diagram and a tabular format.\\n5 Whys, or 5Y, is a powerful tool for getting to the root cause of a problem, and an effective way to expose weaknesses in your systems or processes. ... The tool's simplicity gives it great flexibility, too, and 5 Whys combines well with other methods and techniques, such as Root Cause Analysis.\\nTips for Mastering the 5 Whys Technique Encouraging Open Communication. In mastering the 5 Whys Technique as a problem-solving method, creating an environment that fosters open communication is paramount. When team members feel comfortable expressing their perspectives and insights, it leads to a more comprehensive exploration of the underlying ...\\nRoot cause analysis (RCA) is a common process for discovering the origin of a business problem. While there are many RCA problem-solving techniques, one popular and easy technique is the 5 Whys method. Performing a 5 Whys analysis is one of the most efficient ways to both discover the root cause of a problem and ensure that steps are taken to ...\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['documents'][0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59281235-8085-4d3f-99a4-bf56ac09c69d",
   "metadata": {},
   "source": [
    "# Test Idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54388fb2-5d95-45cd-82eb-1a408d380647",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi there!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8823e6ef-7cd7-455a-8c47-fc747962c522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(base_url=url, model=model, format=\"json\", temperature=0)\n",
    "llm_chain = llm | JsonOutputParser()\n",
    "llm_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88078907-da19-4b54-8038-7041dee23465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{}', response_metadata={'model': 'llama3', 'created_at': '2024-05-27T17:11:39.1536505Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 367388800, 'load_duration': 2491800, 'prompt_eval_duration': 170305000, 'eval_count': 2, 'eval_duration': 192106000}, id='run-e455908d-8754-45cd-a7a2-f761a95db63e-0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "951f427d-86f9-421d-8fc0-86c98a86c08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! It's nice to meet you. Is there something I can help you with, or would you like to chat?\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(base_url=url, model=model, temperature=0)\n",
    "llm_chain = llm | StrOutputParser()\n",
    "llm_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf0b59-b003-4888-9073-e5a261ed68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b88da-19a0-42d0-8d48-8f75c4bac0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a funny assistant who lightens the mood of everyone. Your job is to answer the question in a nice and chill voice.\n",
    "\n",
    "Here is a question: {question} <|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "58c045e2-4e13-4348-a09c-2b587e9e384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a straight-shooter young assistant who live in the wild west. Your job is to answer the question in a lone-star cowboy voice.\n",
    "\n",
    "Here is a question: {question} <|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fccc150c-fc61-443a-9a09-99d930309902",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a fearsome mafia-like assistant who treaten everyone in your sight. Your job is to answer the question in an angry aggressive voice.\n",
    "\n",
    "Here is a question: {question} <|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "61c8a8f0-104b-4d90-88fb-6a7e48e5ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a robin, Batman's assistant who is very clever and loyal to your master. Your job is to answer the question in a thorough, systematic, clever tone of voice.\n",
    "Remember you answer directly to Batman.\n",
    "\n",
    "Here is a question: {question} <|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ee76e69b-050f-412d-b1dc-0f61e9bc51e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Bruce, I mean, Batman. It seems we have an...unusual inquiry on our hands. A greeting, rather than a pressing matter requiring immediate attention. Very well, I shall acknowledge it nonetheless. \"Hi there\" indeed. A most...casual salutation, don't you think? (pauses) Now, if you'll excuse me, Master Bruce, I have more pressing matters to attend to. The night is young, and the streets of Gotham are never truly quiet. Shall we proceed with our usual vigilance and preparedness for whatever darkness may arise?\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm = ChatOllama(base_url=url, model=model, temperature=0)\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "print(llm_chain.invoke(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00235e6e-ca01-432e-9a78-e2490a9df098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Bruce, I've got just the thing for you. The 5 Whys technique, also known as the \"Why Method,\" is a systematic approach to root cause analysis, developed by the legendary quality control expert, Sakichi Toyoda. It's a simple yet powerful tool that helps us get to the heart of a problem by repeatedly asking why until we reach the underlying cause.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. Identify the problem or issue.\n",
      "2. Ask \"Why\" once: What happened?\n",
      "3. Answer with another \"Why\": Why did that happen?\n",
      "4. Repeat step 3 until you've reached the fifth \"Why,\" which should reveal the root cause of the problem.\n",
      "\n",
      "Let me illustrate this with a couple of examples, Master Bruce:\n",
      "\n",
      "Example 1: A malfunctioning Bat-signal\n",
      "\n",
      "* Problem: The Bat-signal isn't working.\n",
      "* Why? (Round 1) Because it's not turning on.\n",
      "* Why is it not turning on? (Round 2) Because the power source is dead.\n",
      "* Why is the power source dead? (Round 3) Because someone accidentally cut the power line.\n",
      "* Why did they accidentally cut the power line? (Round 4) Because they were in a hurry and didn't double-check their work.\n",
      "* Why were they in a hurry? (Round 5) Because they were trying to meet an impossible deadline.\n",
      "\n",
      "In this case, the root cause of the problem is the unrealistic deadline, which led to careless behavior and ultimately, the malfunctioning Bat-signal.\n",
      "\n",
      "Example 2: A mysterious break-in at Wayne Manor\n",
      "\n",
      "* Problem: Someone broke into Wayne Manor.\n",
      "* Why? (Round 1) Because they found an open window.\n",
      "* Why was the window open? (Round 2) Because it wasn't properly secured.\n",
      "* Why wasn't it properly secured? (Round 3) Because someone forgot to lock it after cleaning the room.\n",
      "* Why did they forget to lock it? (Round 4) Because they were distracted by a phone call.\n",
      "* Why were they distracted by a phone call? (Round 5) Because they were worried about Alfred's well-being.\n",
      "\n",
      "In this case, the root cause of the problem is the distraction caused by concern for Alfred's well-being, which led to carelessness and ultimately, the break-in at Wayne Manor.\n",
      "\n",
      "There you have it, Master Bruce. The 5 Whys technique is a valuable tool in our crime-fighting arsenal, helping us get to the bottom of even the most complex mysteries.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.invoke(\"What is 5 whys techniques? Can you explain it to me with one or two examples?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "763b3e38-b9dd-4178-b02b-aa8004676dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = web_search_tool.invoke({\"query\":\"What is 5 whys techniques? Can you explain it to me with one or two examples?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9297e012-ad2b-4e37-8e2b-bfc75ff6159c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', 'content'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d8fd3119-1820-43e0-826c-a7962a6c5b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are six steps to implementing Toyoda's five whys method: 1. Assemble a team. Choose a team that can give you insights into the inner workings of the relevant department or area. You should include management and employees. The best responses come from employees who have first-hand knowledge of the situation.\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de089ee6-773a-43ac-9350-315b53830ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d81c603c-e630-437f-b725-6c3f923b9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2dbd5a5a-a45e-4d6c-804b-097b3e7a847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [url['url'] for url in search_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "09dc289c-cfd5-4996-842a-097513c3a051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.indeed.com/career-advice/career-development/5-whys-example',\n",
       " 'https://reliability.com/resources/articles/5-whys-root-cause-analysis/',\n",
       " 'https://buffer.com/resources/5-whys-process/',\n",
       " 'https://www.masterclass.com/articles/how-to-use-the-5-whys-technique-for-a-root-cause-analysis',\n",
       " 'https://www.mindtools.com/a3mi00v/5-whys']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ac1f3e4a-1b76-4617-b16f-c717a3ec96ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.indeed.com/career-advice/career-development/5-whys-example',\n",
       "  'content': \"Here are six steps to implementing Toyoda's five whys method: 1. Assemble a team. Choose a team that can give you insights into the inner workings of the relevant department or area. You should include management and employees. The best responses come from employees who have first-hand knowledge of the situation.\"},\n",
       " {'url': 'https://reliability.com/resources/articles/5-whys-root-cause-analysis/',\n",
       "  'content': \"Step 1: Identify the Problem. Before diving into a 5 Whys analysis, it's crucial to clearly identify the problem or issue at hand. This step sets the stage for the entire process and ensures that the focus remains on addressing the right concern. Take the time to gather relevant data, observe patterns, and consult with team members or ...\"},\n",
       " {'url': 'https://buffer.com/resources/5-whys-process/',\n",
       "  'content': 'Step 2: Select a 5 Whys master for the meeting. The 5 Whys master will lead the discussion, ask the 5 whys, and assign responsibility for the solutions the group comes up with. The rest of those involved will answer those questions and discuss. In our experience, anyone can be a 5 Whys master ‚Äî there are no special qualifications, and it ...'},\n",
       " {'url': 'https://www.masterclass.com/articles/how-to-use-the-5-whys-technique-for-a-root-cause-analysis',\n",
       "  'content': 'Root cause analysis (RCA) is a common process for discovering the origin of a business problem. While there are many RCA problem-solving techniques, one popular and easy technique is the 5 Whys method. Performing a 5 Whys analysis is one of the most efficient ways to both discover the root cause of a problem and ensure that steps are taken to ...'},\n",
       " {'url': 'https://www.mindtools.com/a3mi00v/5-whys',\n",
       "  'content': \"The model follows a very simple seven-step process: [1] 1. Assemble a Team. Gather together people who are familiar with the specifics of the problem, and with the process that you're trying to fix. Include someone to act as a facilitator, who can keep the team focused on identifying effective counter-measures. 2.\"}]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "077d75ea-aa1a-452e-a91b-a678b0b10e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c381fc0-4d93-4acb-8d45-a7abe642ea5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b59e4e36-d164-498b-92e1-7371eaf5a7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5035178a-d6bb-4ea2-93a1-dc50d21a1c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Just a moment...Enable JavaScript and cookies to continue', metadata={'source': 'https://www.masterclass.com/articles/how-to-use-the-5-whys-technique-for-a-root-cause-analysis', 'title': 'Just a moment...', 'language': 'en-US'})]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0a431f43-fda2-4b01-ad18-220bfc05f0d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='\\n\\n\\n\\n  \\n\\nAttention Required! | Cloudflare\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease enable cookies.\\n\\n\\nSorry, you have been blocked\\nYou are unable to access indeed.com\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhy have I been blocked?\\nThis website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data.\\n\\n\\nWhat can I do to resolve this?\\nYou can email the site owner to let them know you were blocked. Please include what you were doing when this page came up and the Cloudflare Ray ID found at the bottom of this page.\\n\\n\\n\\n\\n\\nCloudflare Ray ID: 88a7f5b15c4d2721\\n‚Ä¢\\n\\n      Your IP:\\n      Click to reveal\\n184.22.35.227\\n‚Ä¢\\n\\nPerformance & security by Cloudflare\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.indeed.com/career-advice/career-development/5-whys-example', 'title': 'Attention Required! | Cloudflare', 'language': 'en-US'})]\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "26e551f1-43ea-486a-9b8b-664bdfe6537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0cf0f66c-ee24-4602-a951-85b8be9a6093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n",
      "Look up things online.\n",
      "{'query': {'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Look up things online.\"\"\"\n",
    "    return \"LangChain\"\n",
    "\n",
    "print(search.name)\n",
    "print(search.description)\n",
    "print(search.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f400dd91-81ec-49f2-b706-1543548f2798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply two integers together.\n",
      "{'first_int': {'title': 'First Int', 'type': 'integer'}, 'second_int': {'title': 'Second Int', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "52ae23db-4eab-4002-bce6-0aec0086f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = agent_llm.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a7dbe43d-1307-48e7-a77b-96335e11f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = agent_llm.bind_tools(\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, \" \"e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3eb038f8-20dc-40c7-9607-c428d3d7db38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}'}}, id='run-e2445e36-b3e2-469c-a152-34337334e65f-0')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm_with_tools.invoke(\"what is the weather in Boston?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce5688-c698-468e-a0bd-055b67d215d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
