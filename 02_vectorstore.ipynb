{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3df42f-284b-456e-86db-97d08aaee2c4",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bd7b7d-8df3-4840-a5ed-1874eec047c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package import embedding, llm, agent_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b14bd1-a7dd-469d-97c3-577ae5adb930",
   "metadata": {},
   "source": [
    "# Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7826852-be50-440d-9ae2-9e3bf99a1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b36e93-dbe9-43bf-ae4e-d01542b72b21",
   "metadata": {},
   "source": [
    "# Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b29bde77-3879-4a5f-9e39-cc2063169c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# load and store you secret api key\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf3b8f-86ed-4599-849e-4c9eff328cac",
   "metadata": {},
   "source": [
    "# UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94ac6c0a-5d0d-4b17-9e71-74b3514577d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edfcd6-9e76-4259-b6c4-905adcdf3be7",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab250c7e-bb5c-478b-b78c-68ad81a2c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "chunk_overlap = 0.1\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=int(chunk_size*chunk_overlap)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de204d8-64c7-4968-8299-03ce1d5aaa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"https://arxiv.org/pdf/2404.19553\")\n",
    "docs = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5ade595-77f6-4327-888f-da4565977563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53bfb0-0656-4b55-b535-812ddfb8b2bd",
   "metadata": {},
   "source": [
    "# Create vectorstore obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7221b71-8a57-493b-b722-55b880f21342",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"extending_context_window_llama_3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab043d0-8d64-42ce-9744-39036548fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89951a34-36dd-4bef-9f6a-bcda067d9ed2",
   "metadata": {},
   "source": [
    "# RAG-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd591cf4-3186-447b-b009-90e277777887",
   "metadata": {},
   "source": [
    "## Define Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345f2e26-9242-4bba-87e7-2bd532d1c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful assistant. Use the available context to answer the question.\n",
    "If you can't answer the question, say \"I don't have enough information\" and don't make your own answer.\n",
    "Discard irrelavant information.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b671742-4b00-4441-9d7c-9eb0bb102a0a",
   "metadata": {},
   "source": [
    "## Define Chain with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03cc3af7-a42e-479c-b318-075610c45b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever | format_docs, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94f75e-0b92-453a-882a-abda1c4f5373",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed1008ff-b7cd-44b9-9a20-144fb7ea4db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = llm_chain.invoke({\"question\": \"what does the 'context' in 'long context' refer to?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89ef4f5-9678-475b-aa96-ec9c046bffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can help you with that!\n",
      "\n",
      "The \"context\" in \"long context\" refers to the maximum number of tokens (or words) that a language model can process or understand at once. In other words, it's the length of the input text that the model can handle before making predictions or generating output.\n",
      "\n",
      "In this specific context, there are mentions of extending the context window beyond 2 million tokens ([8]), scaling language models to 128k context ([9]), and evaluating long-context evaluation beyond 100k tokens ([17]). These references suggest that \"context\" is indeed related to the maximum input length that a model can handle.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd8bce9-59a4-4e24-a63c-d8387b937cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = (\n",
    "    {\"context\": RunnablePassthrough() | retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f05b1662-0770-48fa-926b-e5f1fcd728ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = llm_chain.invoke(\"what does the 'context' in 'long context' refer to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ccbc7ab-8c52-4980-b5a7-54dac033df04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can help you with that!\n",
      "\n",
      "The \"context\" in \"long context\" refers to the maximum number of tokens (or words) that a language model can process or understand at once. In other words, it's the length of the input text that the model can handle before making predictions or generating output.\n",
      "\n",
      "In this specific context, there are mentions of extending the context window beyond 2 million tokens ([8]), scaling language models to 128k context ([9]), and evaluating long-context evaluation beyond 100k tokens ([17]). These references suggest that \"context\" is indeed related to the maximum input length that a model can handle.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e2c16-3fdd-4ecb-bb50-4a771a6934f6",
   "metadata": {},
   "source": [
    "# Retriver method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfce5688-c698-468e-a0bd-055b67d215d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " 'InputType',\n",
       " 'OutputType',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__ror__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '__weakref__',\n",
       " '_abatch_with_config',\n",
       " '_abc_impl',\n",
       " '_acall_with_config',\n",
       " '_aget_relevant_documents',\n",
       " '_atransform_stream_with_config',\n",
       " '_batch_with_config',\n",
       " '_calculate_keys',\n",
       " '_call_with_config',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_expects_other_args',\n",
       " '_get_relevant_documents',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_iter',\n",
       " '_new_arg_supported',\n",
       " '_transform_stream_with_config',\n",
       " 'aadd_documents',\n",
       " 'abatch',\n",
       " 'abatch_as_completed',\n",
       " 'add_documents',\n",
       " 'aget_relevant_documents',\n",
       " 'ainvoke',\n",
       " 'allowed_search_types',\n",
       " 'assign',\n",
       " 'astream',\n",
       " 'astream_events',\n",
       " 'astream_log',\n",
       " 'atransform',\n",
       " 'batch',\n",
       " 'batch_as_completed',\n",
       " 'bind',\n",
       " 'config_schema',\n",
       " 'config_specs',\n",
       " 'configurable_alternatives',\n",
       " 'configurable_fields',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'get_graph',\n",
       " 'get_input_schema',\n",
       " 'get_lc_namespace',\n",
       " 'get_name',\n",
       " 'get_output_schema',\n",
       " 'get_prompts',\n",
       " 'get_relevant_documents',\n",
       " 'input_schema',\n",
       " 'invoke',\n",
       " 'is_lc_serializable',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_id',\n",
       " 'lc_secrets',\n",
       " 'map',\n",
       " 'metadata',\n",
       " 'name',\n",
       " 'output_schema',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'pick',\n",
       " 'pipe',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'search_kwargs',\n",
       " 'search_type',\n",
       " 'stream',\n",
       " 'tags',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'transform',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'validate_search_type',\n",
       " 'vectorstore',\n",
       " 'with_config',\n",
       " 'with_fallbacks',\n",
       " 'with_listeners',\n",
       " 'with_retry',\n",
       " 'with_types']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed054356-4cbf-411e-975d-e74c4cbaa7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what does the 'context' in 'long context' refer to?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caaac73a-f3c8-43c3-b9cb-073d383da873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='[8] Y. Ding, L. L. Zhang, C. Zhang, Y. Xu, N. Shang, J. Xu, F. Yang, and M. Yang. Longrope:\\nExtending llm context window beyond 2 million tokens, 2024.\\n[9] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data engineering for\\nscaling language models to 128k context, 2024.\\n[10] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding, 2021.\\n[11] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\\n[12] D. Li*, R. Shao*, A. Xie, Y. Sheng, L. Zheng, J. E. Gonzalez, I. Stoica, X. Ma, , and H. Zhang.\\nHow long can open-source llms truly promise on context length?, June 2023.\\n[13] OpenAI. Gpt-4 technical report, 2024.', metadata={'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'page': 3, 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240501003931Z', 'modDate': 'D:20240501003931Z', 'trapped': '', '_id': 'a32daadcd8f549359f514c57541f6a8e', '_collection_name': 'extending_context_window_llama_3'})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(query, k=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2a2b3e5-b034-41f9-9f33-16a039f82b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='[8] Y. Ding, L. L. Zhang, C. Zhang, Y. Xu, N. Shang, J. Xu, F. Yang, and M. Yang. Longrope:\\nExtending llm context window beyond 2 million tokens, 2024.\\n[9] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data engineering for\\nscaling language models to 128k context, 2024.\\n[10] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding, 2021.\\n[11] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\\n[12] D. Li*, R. Shao*, A. Xie, Y. Sheng, L. Zheng, J. E. Gonzalez, I. Stoica, X. Ma, , and H. Zhang.\\nHow long can open-source llms truly promise on context length?, June 2023.\\n[13] OpenAI. Gpt-4 technical report, 2024.', metadata={'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'page': 3, 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240501003931Z', 'modDate': 'D:20240501003931Z', 'trapped': '', '_id': 'a32daadcd8f549359f514c57541f6a8e', '_collection_name': 'extending_context_window_llama_3'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(query, k=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84b8e20a-e8be-4d0c-9e3e-55cc2251a514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONTENT_KEY',\n",
       " 'METADATA_KEY',\n",
       " 'VECTOR_NAME',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_aembed_query',\n",
       " '_aembed_texts',\n",
       " '_agenerate_rest_batches',\n",
       " '_asimilarity_search_with_relevance_scores',\n",
       " '_build_condition',\n",
       " '_build_payloads',\n",
       " '_cosine_relevance_score_fn',\n",
       " '_document_from_scored_point',\n",
       " '_embed_query',\n",
       " '_embed_texts',\n",
       " '_embeddings',\n",
       " '_embeddings_function',\n",
       " '_euclidean_relevance_score_fn',\n",
       " '_generate_clients',\n",
       " '_generate_rest_batches',\n",
       " '_get_retriever_tags',\n",
       " '_max_inner_product_relevance_score_fn',\n",
       " '_qdrant_filter_from_dict',\n",
       " '_select_relevance_score_fn',\n",
       " '_similarity_search_with_relevance_scores',\n",
       " 'aadd_documents',\n",
       " 'aadd_texts',\n",
       " 'aconstruct_instance',\n",
       " 'add_documents',\n",
       " 'add_texts',\n",
       " 'adelete',\n",
       " 'afrom_documents',\n",
       " 'afrom_texts',\n",
       " 'amax_marginal_relevance_search',\n",
       " 'amax_marginal_relevance_search_by_vector',\n",
       " 'amax_marginal_relevance_search_with_score_by_vector',\n",
       " 'as_retriever',\n",
       " 'asearch',\n",
       " 'asimilarity_search',\n",
       " 'asimilarity_search_by_vector',\n",
       " 'asimilarity_search_with_relevance_scores',\n",
       " 'asimilarity_search_with_score',\n",
       " 'asimilarity_search_with_score_by_vector',\n",
       " 'async_client',\n",
       " 'client',\n",
       " 'collection_name',\n",
       " 'construct_instance',\n",
       " 'content_payload_key',\n",
       " 'delete',\n",
       " 'distance_strategy',\n",
       " 'embeddings',\n",
       " 'from_documents',\n",
       " 'from_existing_collection',\n",
       " 'from_texts',\n",
       " 'max_marginal_relevance_search',\n",
       " 'max_marginal_relevance_search_by_vector',\n",
       " 'max_marginal_relevance_search_with_score_by_vector',\n",
       " 'metadata_payload_key',\n",
       " 'search',\n",
       " 'similarity_search',\n",
       " 'similarity_search_by_vector',\n",
       " 'similarity_search_with_relevance_scores',\n",
       " 'similarity_search_with_score',\n",
       " 'similarity_search_with_score_by_vector',\n",
       " 'vector_name']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(retriever.vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cb13ef9-2106-4fa1-9026-ec54e5391499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='[8] Y. Ding, L. L. Zhang, C. Zhang, Y. Xu, N. Shang, J. Xu, F. Yang, and M. Yang. Longrope:\\nExtending llm context window beyond 2 million tokens, 2024.\\n[9] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data engineering for\\nscaling language models to 128k context, 2024.\\n[10] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding, 2021.\\n[11] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\\n[12] D. Li*, R. Shao*, A. Xie, Y. Sheng, L. Zheng, J. E. Gonzalez, I. Stoica, X. Ma, , and H. Zhang.\\nHow long can open-source llms truly promise on context length?, June 2023.\\n[13] OpenAI. Gpt-4 technical report, 2024.', metadata={'source': 'https://arxiv.org/pdf/2404.19553', 'file_path': 'https://arxiv.org/pdf/2404.19553', 'page': 3, 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240501003931Z', 'modDate': 'D:20240501003931Z', 'trapped': '', '_id': 'a32daadcd8f549359f514c57541f6a8e', '_collection_name': 'extending_context_window_llama_3'}),\n",
       " 0.5891520825008985)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search_with_score(query, k=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dad057-3726-4d6e-b913-ea1d8bf56189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
